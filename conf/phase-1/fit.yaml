data: data.yaml
tokenizer: ../tokenizer.yaml
logger: logger.yaml
trainer:
  max_steps: 60000
  log_every_n_steps: 10
  precision: bf16-true
  gradient_clip_val: 4
  gradient_clip_algorithm: norm
  save_embedding_layers: true
  callbacks:
  - class_path: luolib.lightning.callbacks.ModelCheckpoint
    init_args:
      every_n_train_steps: 2500
      filename: "{step}"
      save_top_k: -1
      save_last: true
      verbose: true
#  strategy:
#    class_path: lightning.pytorch.strategies.DDPStrategy
#    init_args:
#      timeout: 0:0:45
optim:
  sam:
    prefix: [sam_model, seg_proj]
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-4
        weight_decay: 5e-2
    lr_scheduler:
      scheduler:
        class_path: timm.scheduler.CosineLRScheduler
        init_args:
          t_initial: ${trainer.max_steps}
          t_in_epochs: false
          warmup_t: 200
          warmup_prefix: true
      interval: step
      frequency: 200
  default:
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 5e-5
        weight_decay: 5e-2
    lr_scheduler:
      scheduler:
        class_path: timm.scheduler.CosineLRScheduler
        init_args:
          t_initial: ${trainer.max_steps}
          t_in_epochs: false
          warmup_t: 0
          warmup_prefix: true
      interval: step
      frequency: 200
mask_loss:
  lambda_focal: 1
