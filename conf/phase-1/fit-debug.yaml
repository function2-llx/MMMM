data:
  patch_size: [48, 224, 224]
  dataloader:
    train_batch_size: 4
tokenizer: ../tokenizer-debug.yaml
logger: logger-debug.yaml
trainer:
  max_steps: 60000
  log_every_n_steps: 10
  precision: bf16-true
  gradient_clip_val: 1
  gradient_clip_algorithm: norm
  callbacks:
  - class_path: luolib.lightning.callbacks.ModelCheckpoint
    init_args:
      every_n_train_steps: 2500
      filename: "{step}"
      save_top_k: -1
      save_last: true
      verbose: true
optim:
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      weight_decay: 5e-2
  lr_scheduler:
    scheduler:
      class_path: timm.scheduler.CosineLRScheduler
      init_args:
        t_initial: ${trainer.max_steps}
        t_in_epochs: false
        warmup_t: 0
        warmup_prefix: true
    interval: step
    frequency: 200
mask_loss:
  lambda_focal: 1
model:
  class_path: mmmm_debug.model.from_debug_sam
  init_args:
    pretrained_model_name_or_path: THUDM/cogvlm-chat-hf
    vlm_override:
      patch_size: [8, 16, 16]
      pos_embed_shape: [6, 14, 14]
      pt_pos_embed_shape: [35, 35]
    sam:
      patch_size: [8, 16, 16]
      pos_embed_shape: [6, 14, 14]
      pt_in_channels: 1
      pt_patch_size: [4, 16, 16]
      pt_pos_embed_shape: [8, 16, 16]
      checkpoint: pre-trained/SegVol_v1.pth
