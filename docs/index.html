<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:32px">One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</span><br><br><br>
	</center>

        <table align="center" width="640px">
            <tbody><tr>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Ziheng Zhao</a><sup>1,2</sup></span>
                </center>
                </td>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Yao Zhang</a><sup>1,2</sup></span>
                </center>
                </td>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a><sup>1,2</sup></span>
                </center>
                </td>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a><sup>1,2</sup></span>
                </center>
                </td>

            </tr>

        </tbody></table><br>

        <table align="center" width="480px">
            <tbody><tr>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
                </td>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2</sup></span>
                </center>
                </td>

                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2</sup></span>
                </center>
                </td>

            </tr>

        </tbody></table><br>

	
	      <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
        </tr></tbody></table>
	
	    <table align="center" width="800px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/zhaoziheng/SAT"> [GitHub]</a>
                  </span>
                </center>
              </td>

	        <td align="center" width="200px">
        	<center>
                  <br>
                  <span style="font-size:20px">Data
                    <a href="https://github.com/zhaoziheng/SAT-DS/"> [Data]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper  <a href="https://arxiv.org/abs/2312.17183"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
	
      <br><hr>
      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <left>
	      In this study, we focus on building up a model that aims to <b>S</b>egment <b>A</b>nything in medical scenarios, driven by <b>T</b>ext prompts, termed as <b>SAT</b>. 
        Our main contributions are three folds: 
        (i) for dataset construction, we combine multiple knowledge sources to construct the first multi-modal knowledge tree on human anatomy, 
        including 6502 anatomical terminologies; Then we build up the largest and most comprehensive segmentation dataset for training, 
        by collecting over 22K 3D medical image scans from 72 segmentation datasets with careful standardization on both image scans and label space; 
        (ii) for architecture design, we formulate a universal segmentation model, 
        that can be prompted by inputting medical terminologies in text form. 
        We present knowledge-enhanced representation learning on the combination of a large number of datasets; 
        (iii) for model evaluation, we train a <b>SAT-Pro</b> with only <b>447M parameters</b>, 
        to segment <b>72</b> different segmentation datasets with text prompt, resulting in <b>497</b> classes. 
        We have thoroughly evaluated the model from three aspects: averaged by body regions, averaged by classes, and average by datasets, 
        demonstrating comparable performance to <b>72 specialist nnU-Nets</b>, i.e., we train nnU-Net models on each dataset/subset, 
        resulting in 72 nnU-Nets with around <b>2.2B parameters</b> for the 72 datasets. 
        We will release all the codes, and models in this work.
        </left></p>
        <table align="center" width="800px">
              <tbody><tr>
                <td align="center" width="600px">
                  <center>
                    <img style="width:800px" src='./resources/new_teaser.pdf'></img>
                  </center>
                </td>
              </tr></tbody>
        </table>
	
      <br><hr>
      <center> <h2> Datasets </h2> </center>
      <p><b><h3>Domain Knowledge</h3></b></p>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
	     We construct a knowledge tree based on multiple medical knowledge source. 
       It encompassing thousands of anatomy concepts and definitions throughout the human body. 
       They are linked via the relations and additionally, some are further mapped to segmentations on the atlas images, 
       demonstrating their visual features that may hardly be described purely by text.
	    </left></p>
      <table align="center" width="800px">
          <tbody><tr>
            <td align="center" width="600px">
              <center>
                <img style="width:800px" src='./resources/knowledge_source.pdf'></img>
              </center>
            </td>
          </tr></tbody>
      </table>

      <p><b><h3>Segmentation Datasets</h3></b></p>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
       To equip our universal segmentation model with the ability to handle segmentation tasks of different targets across various modalities and anatomical regions, 
       we collect and integrate 72 diverse publicly available medical segmentation datasets, 
       totaling 22,186 scans including both CT and MRI and 302,033 segmentation annotations, 
       covering 497 anatomical structures and lesions spanning 8 different regions of the human body: Brain, Head and Neck, Upper Limb, Thorax, Abdomen, Plevis, and Lower Limb. 
       The dataset collection is termed as <b>SAT-DS</b>. 
       Detailed composition of the dataset are present in the paper.
      </left></p>
      <table align="center" width="800px">
          <tbody><tr>
            <td align="center" width="600px">
              <center>
                <img style="width:800px" src='./resources/wholebody_demonstration.pdf'></img>
              </center>
            </td>
          </tr></tbody>
      </table>
      <br>
      <hr>

      <center><h2>Method</h2></center>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
       Towards building up our universal segmentation model by text prompt, i.e., SAT, we consider two main stages, namely, <b>multimodal knowledge injection</b> (a) and <b>universal segmentation training</b> (b). 
       In the first stage, we pair the data in the constructed knowledge tree into text-text or text-atlas segmentation pairs, 
       and use them for visual-language pre-training, 
       injecting rich multimodal medical domain knowledge into the visual and text encoders;
       In the second stage, we build a universal segmentation model prompted by text. 
       Here, the pretrained text encoder is applied to generate embedding for any anatomical target terminology as the text prompt for segmentation.
      </left></p>
      <table align="center" width="800px">
          <tbody><tr>
            <td align="center" width="800px">
              <center>
                <img style="width:800px" src='./resources/framework_wcy.pdf'></img>
              </center>
            </td>
          </tr></tbody>
      </table>
      <br>
      <hr>

      <center><h2>Result</h2></center>

      <p><b>A universal model is worth 72 specialist models</b></p> 
      We evaluate SAT-Nano and SAT-Pro on all the 72 segmentation datasets in the collection. 
      As there is no appropriate benchmark for evaluating the universal segmentation model, 
      we randomly split each dataset in the collection into 80% for training and 20% for testing.
      We take nnU-Net as a strong baseline for comparison, and train specialist nnU-Net model on each of the datasets, 
      resulting in 72 nnU-Net models with optimized configuration on each dataset.
      And the results are compared region-wise, class-wise and dataset-wise respectively.
      Here, we present the comparison with 72 nnU-Net models on each region, and model size. 
      SAT-Pro shows comparable performance to the combination of 72 nnU-Net models, with only <b>1/5</b> model size.
      For class-wise and dataset-wise results, 
      please refer to the supplementary materials in the paper.

      <table align="center" width="800px">
            <tbody><tr>
              <td align="center" width="600px">
                <center>
                  <img style="width:800px" src='./resources/radar_v2.pdf'></img>
                </center>
              </td>
            </tr></tbody>
      </table>
      <table align="center" width="800px">
        <tbody><tr>
          <td align="center" width="600px">
            <center>
              <img style="width:800px" src='./resources/region_table.png'></img>
            </center>
          </td>
        </tr></tbody>
      </table>

      <p><b>Domain knowledge can boost segmentation performance</b> </p> 
      We configure and train three SAT-Nano variants, with different text encoders: 
      The text encoder pre-trained on our multimodal medical knowledge graph; 
      MedCPT, the state-of-the-art text encoder for various medical language tasks; 
      And BERT-Base, a prevalent text encoder in natural language processing but not specifically finetuned on medical corpus.
      With respect to the severe long-tail distribution of SAT-DS, 
      we demonstrate the knowledge injection we proposed can significantly improve the segmentation performance, 
      especially on the tail classes.
      For detailed results, please refer to Section 2.2 in our paper.
      </p>
      <table align="center" width="800px">
        <tbody><tr>
          <td align="center" width="600px">
            <center>
              <img style="width:700px" src='./resources/long_tail.pdf'></img>
            </center>
          </td>
        </tr></tbody>
      </table>

      <p><b>Zero-shot transfer to real clinic data</b></p> 
      On several in-house clinical images with manual reports,
      we utilize GPT-4 to directly extract the anatomical targets of interests from reports 
      and prompt SAT-Pro to segment them on the image. 
      This demonstrate the potentail of SAT as a powerful agent for LLMs, 
      and its powerful generalization capability to real clinic data, despite the diversity of images and segmentation targets.
      </p>
      <table align="center" width="800px">
        <tbody><tr>
          <td align="center" width="600px">
            <center>
              <img style="width:700px" src='./resources/zero-shot-six1.pdf'></img>
            </center>
          </td>
        </tr></tbody>
      </table>
      <table align="center" width="800px">
        <tbody><tr>
          <td align="center" width="600px">
            <center>
              <img style="width:700px" src='./resources/zero-shot-six2.pdf'></img>
            </center>
          </td>
        </tr></tbody>
      </table>

      
      <center> <h2> Acknowledgements </h2> </center>
      <p> 
	      Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>


<br>
</body>
</html>
